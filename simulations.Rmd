---
title: "Voortgang simulatiestudie"
author: "Steven Wallaert"
date: "`r Sys.Date()`"
output: 
  html_document:
    code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, tidy=TRUE)
```




```{r}
library(tidyverse)

```




In de voorbije tijd ben ik bezig geweest met het uitschrijven van code voor de simulatie.

Om een idee te geven van waar ik sta en van hoe de output er momenteel uit kan zien heb ik 4 condities gesimuleerd met de logistic lasso. Deze condities zijn allen: n=10, significante features=1, aantal simulaties=400. Ik heb telkens p laten variëren over $\{10, 30, 100, 300, 1000, 3000, 6000 \}$ en d laten variëren over $\{0.36, 0.74, 1.19, 1.81\}$, geen correlaties tussen de features.

Aantal simulaties heb ik gekozen op basis van de breedte van een 95% CI voor een proportie van .5. Ik heb dit vrij arbitrair gekozen, nl. een maximale breedte van .1 oftewel een maximale SE van ongeveer .025. 400 simulaties (met alle waarden voor p, 1 waarde voor d) resulteert momenteel in een rekentijd van ongeveer 2 uur.

De verschillende waarden voor p heb ik zo gekozen zodat alle grootteordes zouden vertegenwoordigd zijn. Ik ondervond wel problemen bij p > 6000. Bijvoorbeeld bij p=1e4 loopt mijn pc vast. Wellicht heeft dit meer te maken met de manier waarop de code geschreven is dan met mijn pc, al is die niet meer van de jongste. Toch, vorig jaar bij het project van het vak High Dimensional Data Analysis lukte het om de dataset te analyseren waarbij p in de grootteorde lag van om en bij de 5e4 (al duurde dit ook wel enkele uren). Voorlopig heb ik dus 6000 ipv 1e4 genomen.

De verschillende waarden voor d heb ik gekozen op basis van de definitie van AUC zoals besproken in ons laatste gesprek. 
Ik hoop dat ik het juist heb, ik koos voor volgende AUC waarden: $\{0.6, 0.7, 0.8, 0.9\}$ wat volgens $d=-\sqrt{2\sigma^2}\Phi^{-1}(AUC)$ resulteert in $\{0.36, 0.74, 1.19, 1.81\}$ (ik ga ervan uit dat het teken er niet toe doet). De bedoeling is om met deze waarden te werken voor d over alle condities heen.

Hieronder mijn code, met daaronder de grafische resultaten. Dit alles om te laten weten waar ik sta. Uiteraard kijk ik uit naar opmerkingen, feedback, suggesties en dergelijke.

Alvast heel erg bedankt.

## Code

Ik heb hieronder mijn code geplakt. Op mijn pc staan de verschillende functies in verschillende scripts opgeslagen.

De code is hier en daar al aangepast geweest (meestal om de code sneller te maken, al is dat natuurlijk relatief. Ik besef dat dit wellicht niet-zo-optimale code is wat betreft performantie). Meestal laat ik oude code in comments, zo is het duidelijk 'van waar ik kom'.
Verder heb ik geprobeerd om de code ruim te voorzien van comments zodat het duidelijk wordt wat de code beoogt.

### create_data

De naam zegt het, hiermee maak ik de data aan.

```{r, eval=FALSE}
# function to simulate data:
# 
# 
# n - number of samples
# p - number of features
# sig_p - number of significant features, predictive of response
# d - difference between 2 groups in the significant features
# no correlation structure (for now)
# all gaussian

create_data <- function(n, p, sig_p, d){
  require(tidyverse)
  n_target <- n %/% 2 # as of now: equal n target-control
  n_control <- n - n_target
  
  # idea is to randomize the order of the target and controls
  # not sure if this is needed though
  in_order <- c(rep(1, n_target), rep(0, n_control))
  Y <- sample(in_order, size = n, replace = FALSE)
  
  # keep track of the significant features
  significant_features <- seq_len(sig_p) # seq_len: covers the case where sig_p = 0

  # create X following X_i ~ N(Y_i*d, sigma = 1)
  X <- sapply(rep(0, p), function(x) rnorm(n, x, 1))
  X[, significant_features] <- X[, significant_features] + Y*d

  # output in several formats
  # XY <- as_tibble(X, .name_repair = "unique") %>%
  #   add_column(Y)
  # YX <- tibble(Y, as_tibble(X, .name_repair = "unique"))
  
  list(X=X, Y=Y, sig_features = significant_features)
}


```

### run_experiment

Deze functie zorgt voor het doorlopen van 1 individuele conditie, dus voor 1 bepaalde n,p, sig_p, d (en nsims).

Bij dit deel heb ik nog een specifieke vraag: Oorspronkelijk was ik van plan volgende waarden voor de AUC te berekenen:

- De $\widehat{AUC}$ die berekend wordt op de (eerder beperkte) test set (waarvan de data uiteraard niet gebruikt werd om het model te fitten)
- De populatie-AUC, die berekend wordt op een onafhankelijke grote test set (in mijn geval n=1e4, bij grotere ordes loopt m'n pc vast). Dit berekenen wordt gedaan op basis van het model dat gefit werd op de training set.

Tijdens het vak Big Data zagen we dat het de gewoonte is om na het evalueren van het model op de test set nog eens een finaal model te fitten, maar dan met de volledige data (ook de data die gebruikt werd om hyperparameters te optimaliseren etc.). Dit zette mij er toe aan om naast de vorige maten ook nog eens de populatie-AUC te berekenen maar dan gebruik makend van een model dat gefit werd op de volledige dataset. De vraag is of dit ook _common practice_ is binnen de context van biomarker discovery? Ik ben het tijdens het lezen nog niet echt expliciet tegengekomen.


```{r, eval=FALSE}
# function to run experiment 
# currently only supporting logistic lasso
#
#
# nsims - number of repetitions

source("functions/create_data.R")

run_experiment <- function(n, p, sig_p, d, nsims){
  suppressMessages(suppressWarnings({
    require(glmnet)
    require(ROCR)
    require(doParallel)
    ncores <- detectCores() - 1
    # parameters
    # prop_train: proportion train - test
    # pop_size: population size
    prop_train <- 2/3
    pop_size <- 1e4
    
    #create an empty dataframe to store the results of current condition
    results <- tibble(i = vector(length = nsims), # number of repetition
                      detections = vector(length = nsims), # coefficients dataframe
                      sig_features = vector(length = nsims), # the significant features
                      p = vector(length = nsims),  # number of features
                      measure_cv = vector(length = nsims), # which measure was used: auc or something else?
                      perf_test = vector(length = nsims), # performance result on test set
                      perf_pop_lim = vector(length = nsims), # performance result on population set
                      perf_pop_full = vector(length = nsims) # performance model built on full dataset on population set
    )
    
    
    # create a population data set to test the models on
    pop <- create_data(pop_size, p, sig_p, d)
    
    # nsims repetitions
    pb <- txtProgressBar(min = 0, max = nsims, style = 3)
    for (i in seq_len(nsims)){
      
      #### Data setup ####
      # create simulation data set
      #simdat <- create_data(n, p, sig_p, d)
      
      # train - test split, stratified
      # X_train <- simdat$XY %>%
      #   group_by(Y) %>%
      #   slice_sample(prop = prop_train) %>% 
      #   ungroup()
      
      # separate Y from X
      # Y_train <- X_train %>% select(Y) %>% as.matrix()
      
      # delete Y out of dataframe, don't convert to matrix yet, will need it (cf anti_join)
      # X_train <- X_train %>% select(!Y)
      
      # create test set, use all observations not in train
      # X_test <- simdat$XY %>%
      #   anti_join(X_train, by = "...1")
      # 
      # # now convert X_train to matrix
      # X_train <- X_train %>% as.matrix()
      # 
      # # separate Y from X in test
      # Y_test <- X_test %>% select(Y) %>% as.matrix()
      # 
      # # delete Y out of dataframe and convert to matrix
      # X_test <- X_test %>% select(!Y) %>% as.matrix()
      
      ### better data setup ###
      # profiling pointed out that the previous was slow
      simdat <- create_data(n, p, sig_p, d)
      
      # stratified sampling -> train - test
      train_ind <-
        tibble(Y=simdat$Y) %>%
        rownames_to_column() %>%
        group_by(Y) %>%
        slice_sample(prop = prop_train) %>%
        pull(rowname) %>%
        as.numeric()
      
      Y_train <- simdat$Y[train_ind]
      
      X_train <- simdat$X[train_ind,]
      
      X_test <- simdat$X[-train_ind,]
      
      Y_test <- simdat$Y[-train_ind]
      
      #### logistic lasso ####
      # step 1: use 5-fold CV to obtain optimal value for lambda
      
      cl <- makeCluster(ncores) # added parallel support because of long computation times
      registerDoParallel(cl)
      
      # parallel only profitable when p > 1000 (after experimenting)
      if(p > 1000) {
        para <- TRUE
      } else {
        para <- FALSE
      }
      
      ## the while loop and try construction is there to catch errors in case of extremely small sample sizes
      ## in those cases it happens that some folds have 0 observations in one of the 2 classes, result: error
      cvfit <- 1 # just a random variable so that the program can enter the while loop
      
      while(class(cvfit) != "cv.glmnet"){
        # catches the error in case of an error, or the result in case of a successful try
        cvfit <- try({
          cv.glmnet(X_train, Y_train, family = "binomial", alpha = 1, nfolds = 5, type.measure = "auc", parallel = para)
        }, silent = TRUE) 
      }
      stopCluster(cl)
      # step 2: fit model using optimal value for lambda obtained in step 1
      fit <- glmnet(X_train, Y_train, family = "binomial", alpha = 1, lambda = cvfit$lambda.min)
      
      #### Performance ####
      # step 3: performance on test set
      pred <- prediction(predict(fit, newx = X_test, type = "response"), Y_test)
      results$perf_test[i] <- performance(pred, "auc")@y.values[[1]]
      
      # step 4: performance on population set, trained only on training set, hence limited
      pred_pop <- prediction(predict(fit, newx = pop$X, type = "response"), pop$Y)
      results$perf_pop_lim[i] <- performance(pred_pop, "auc")@y.values[[1]]
      
      # step 5: performance on population set, trained on full set, hence full
      fullfit <- glmnet(simdat$X, simdat$Y, family = "binomial", alpha = 1, lambda = cvfit$lambda.min)
      pred_pop_full <- prediction(predict(fullfit, newx = pop$X, type = "response"), pop$Y)
      results$perf_pop_full[i] <- performance(pred_pop_full, "auc")@y.values[[1]]
      
      #### Save results ####
      # extract coefficients and store in results dataframe
      # Question: using full fit or not?
      fit_coef <- coef(fit)
      
      results$detections[i] <- list(names(fit_coef[fit_coef[,1] != 0, ])) # extract names nonzero features
      
      # repetition                                 
      results$i[i] <- i
      
      # for reference, the significant features
      results$sig_features[i] <- list(sapply(simdat$sig_features, function(x) paste0("V", x)))
      
      # for reference, number of features
      results$p[i] <- p
      
      # although "auc" is chosen as type.measure in cv.glmnet, this is not always possible
      # it may be good to keep track of this as well
      results$measure_cv[i] <- cvfit$name
      
      # print progress to the console
      setTxtProgressBar(pb, i)
    }
    close(pb)
  }))
  results
}


```

### gather_results

Volgende functie zou beter 'gather_results' heten, maar voorlopig heeft ze nog een andere naam.

Wat ze doet is de belangrijkste informatie uit de resultaten van een experiment trekken en deze opslaan in verschillende tabellen.

```{r, eval=FALSE}
# process outputs


detections_summary <- function(results_of_experiment){
  nsims <- dim(results_of_experiment)[1]
  
  # count True and False detections
  #
  # Result = table with count of TRUE and FALSE detections per simulation (i)
  #
  true_false_detections <- results_of_experiment %>%
    rowwise() %>%
    mutate(detections = ifelse(is.null(detections), list(NA), list(detections))) %>% # fill empty detections
    unnest(detections) %>%
    rowwise() %>%
    mutate(detection = factor(detections %in% sig_features,   # find true detections
                              levels=c("TRUE", "FALSE"))) %>% # note that i convert to factor, this is to be able to
    group_by(i) %>%                                           # 'complete' later on. Otherwise this can be a hindrance
    count(detection) %>%                                      # when no TRUE or FALSE detections are made in the entire run
    ungroup() %>%
    mutate(n = ifelse(detection=="TRUE", n, n-1)) %>% # subtract intercept from FALSE detections, TRUEs are left as is
    complete(i, detection, fill = list(n = 0)) # add explicit 0-counts where no detections are made
    

  
  # barplot - not used later on so it is commented out for now
  # barplot_detections <- true_false_detections %>%
  #   ggplot(aes(x=n, y = ..prop.., fill = detection)) +
  #   geom_bar(position = "dodge", stat = "count") +
  #   facet_grid(detection~.)
  
  # detection summary
  #
  # Result = table showing the mean nr of TRUE/FALSE detections + MCSE
  #                   and the proportion of sims that have a TRUE/FALSE detection + MCSE
  #
  detection_summary <- true_false_detections %>%
    group_by(detection) %>% # for TRUE and FALSE detections:
    summarise(mean_nr = mean(n),           # mean number of detections
              mean_se = sd(n)/sqrt(nsims), # MCSE
              a_detection = mean(n>0),     # proportion of simulations that result in 'a detection' 
              a_det_se = sqrt(a_detection * (1-a_detection)/nsims)) # MC SE of proportion
  
  # performance summary
  #
  # Result = table showing mean test auc together with 'true auc' + MCSE
  #          also the sd (+MCSE) of the test auc is caluclated ~ indicates the variance of the estimator
  #          bias is derived (+MCSE)
  #          population auc/bias is derived for models built on training data (lim, l) or full data (full, f)
  performance_summary <- results_of_experiment %>%
    summarise(across(starts_with("perf"), list(auc = mean), .names = "{.col}_{.fn}"), # calculates the mean auc (test, pop_l, pop_f)
              test_se = sd(perf_test)/sqrt(nsims),
              pop_lim_se = sd(perf_pop_lim)/sqrt(nsims),
              pop_full_se = sd(perf_pop_full)/sqrt(nsims),
              auc_sd = sd(perf_test),                        # to have an indication of the variance of the estimator, sd for ease of interpretation
              auc_sd_se = sd(perf_test)/sqrt(2*(nsims-1)),         # MCSE of SD
              bias_l = mean(perf_test - perf_pop_lim),
              bias_l_se = sd(perf_test - perf_pop_lim)/sqrt(nsims),
              bias_f = mean(perf_test - perf_pop_full),
              bias_f_se = sd(perf_test - perf_pop_full)/sqrt(nsims)) %>%
    rename_with(~ str_remove(.x, "perf_"))
  
  list(counts = true_false_detections,
       #barplot = barplot_detections,
       detections = detection_summary,
       performances = performance_summary,
       raw_results = results_of_experiment)
}

```



# Results

Ik heb een aantal verschillende manieren van visualiseren uitgeprobeerd omdat ik weet dat 'dynamite plots' niet altijd gesmaakt worden bij het visualiseren van proporties/gemiddelden. 

```{r}
files <- dir("output/")
files_detect <- files[str_detect(files, "detect.csv$")]

df_detect <- map_df(files_detect, function(x){
  read_csv(paste0("output/", x))
})

files_perf <- files[str_detect(files, "perf.csv$")]
df_perf <- map_df(files_perf, function(x){
  read_csv(paste0("output/", x))
})

files_null <- files[str_detect(files, "null.csv$")]
df_null <- map_df(files_perf, function(x){
  read_csv(paste0("output/", x))
})
```

## Detections

### mean number of detections

```{r, fig.width=10}
df_detect %>%
  #filter(! method %in% c("svm", "tweedie")) %>%
  ggplot(aes(x=p, y=mean_nr, fill=detection)) +
  geom_col(position = "dodge", show.legend = F) +
  geom_errorbar(aes(ymin=mean_nr-1.96*mean_se, ymax=mean_nr+1.96*mean_se)) +
  scale_x_log10(breaks=c(10,30,100,300,1000,3000,1e4, 3e4))  +
  facet_grid(detection~method, scales="free")

df_detect %>%
  #filter(! method %in% c("svm", "tweedie")) %>%
  filter(p < 3000) %>%
  ggplot(aes(x=p, y=mean_nr, group=method)) +
  geom_point(aes(color=method)) +
  geom_line(aes(color=method)) +
  facet_grid(detection~., scales = "free_y") +
  scale_x_log10(breaks=c(10,30,100,300,1000,3000,1e4, 3e4)) +
  geom_linerange(aes(ymin=mean_nr-1.96*mean_se, ymax=mean_nr+1.96*mean_se, color=method), alpha = 0.8) +
  scale_color_brewer(palette = "Set1") +
  lims(y=c(0,10))
```


```{r}
df_detect %>%
  filter(d==0.36) %>%
  ggplot(aes(x=p, y=mean_nr, fill=detection)) +
  geom_col(show.legend = F) +
  geom_errorbar(aes(ymin=mean_nr-1.96*mean_se, ymax=mean_nr+1.96*mean_se)) +
  scale_x_log10(breaks=c(10,30,100,300,1000,3000,6000)) +
  facet_grid(detection~method, scales="free_y")
  
```


### proportion of 'a detection'

```{r, fig.width=10}
df_detect %>%
  ggplot(aes(x = p, y=a_detection, fill=detection)) +
  geom_col(show.legend = F) +
  geom_errorbar(aes(ymin =a_detection - 1.96*a_det_se, ymax= a_detection + 1.96*a_det_se)) +
  scale_x_log10(breaks=c(10,30,100,300,1000,3000,1e4,3e4)) +
  facet_grid(detection~method, scales = "free_y")
```

Idem, maar dan met aangepaste CI's, en andere manier van visualiseren

```{r fig.width=10}
logit <- function(x) log(x/(1-x))
expit <- function(x) exp(x)/(1+exp(x))
lbound <- function(p, n) expit(logit(p) - 1.96/sqrt(n*p*(1-p)))
ubound <- function(p, n) expit(logit(p) + 1.96/sqrt(n*p*(1-p)))

df_detect %>%
  ggplot(aes(x = p, y=a_detection, fill=detection)) +
  geom_point(show.legend = F) +
  geom_line(aes(group = detection), linetype="dashed") +
  geom_errorbar(aes(ymin =lbound(a_detection, nsims), ymax= ubound(a_detection, nsims))) +
  scale_x_log10(breaks=c(10,30,100,300,1000,3000,6000)) +
  facet_grid(detection~d, scales = "free_y")
```


## AUC

```{r, fig.height=6}
df_perf %>%
  pivot_longer(cols=ends_with("auc"), names_to = "set", values_to = "auc") %>%
  ggplot(aes(x=p, y=auc, color=set)) +
  geom_point() +
  geom_line(linetype="dashed") +
  facet_grid(d~.) +
  scale_x_log10(breaks=c(10,30,100,300,1000,3000,6000)) 

```

## Bias: AUC_hat - AUC 

```{r, fig.height=6}
df_perf %>%
  ggplot(aes(x=p, y=bias_l)) +
  geom_hline(yintercept = 0, color = "darkgreen") +
  geom_point() +
  geom_errorbar(aes(ymin=bias_l-1.96*bias_l_se, ymax=bias_l+1.96*bias_l_se), width=0.05) +
  geom_line(linetype="dashed") +
  facet_grid(d~.) +
  scale_x_log10(breaks=c(10,30,100,300,1000,3000,6000)) +
  lims(y=c(-0.1,0.1))
```

## SD(auc_hat)

```{r}
df_perf %>%
  ggplot(aes(x=p, y=auc_sd)) +
  geom_point(aes(color = factor(d))) +
  scale_x_log10(breaks=c(10,30,100,300,1000,3000,6000)) +
  geom_line(aes(color = factor(d), group=d)) +
  geom_ribbon(aes(ymin=auc_sd-1.96*auc_sd_se, ymax=auc_sd+1.96*auc_sd_se, group=d, fill=factor(d)), alpha=0.2)
```

