@article{acharjee2020, 
  title={A random forest based biomarker discovery and power analysis framework for diagnostics research}, 
  volume={13}, DOI={10.1186/s12920-020-00826-6}, 
  number={1}, 
  journal={BMC Medical Genomics}, 
  author={Acharjee, Animesh and Larkman, Joseph and Xu, Yuanwei and Cardoso, Victor Roth and Gkoutos, Georgios V.}, 
  year={2020}
} 
 
@misc{ICsims, 
  title={MarkJBrewer/ICsims: Simulations for studying prediction performance of different INFORMATION CRITERIA}, 
  url={https://github.com/MarkJBrewer/ICsims}, 
  journal={GitHub}, 
  author={Mark J. Brewer}
} 
  
@Manual{Rbase,
  title = {R: A Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  organization = {R Foundation for Statistical Computing},
  address = {Vienna, Austria},
  year = {2021},
  url = {https://www.R-project.org/},
}

@Article{Boruta,
  title = {Feature Selection with the {Boruta} Package},
  author = {Miron B. Kursa and Witold R. Rudnicki},
  journal = {Journal of Statistical Software},
  year = {2010},
  volume = {36},
  number = {11},
  pages = {1--13},
  url = {http://www.jstatsoft.org/v36/i11/},
}

@article{svmrfe,
    title = {Multiple SVM-RFE for gene selection in cancer classification with expression data.},
    volume = {4},
    number = {3},
    journal = {IEEE transactions on nanobioscience},
    author = {Duan, Kai-Bo and Rajapakse, Jagath C and Wang, Haiying and Azuaje, Francisco},
    month = sep,
    year = {2005},
    note = {Original Multiple SVM-RFE paper.},
    pages = {228-234}
}

@Article{glmnet,
  title = {Regularization Paths for Generalized Linear Models via Coordinate Descent},
  author = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
  journal = {Journal of Statistical Software},
  year = {2010},
  volume = {33},
  number = {1},
  pages = {1--22},
  url = {https://www.jstatsoft.org/v33/i01/},
}

@article{BH1995,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346101},
 abstract = {The common approach to the multiplicity problem calls for controlling the familywise error rate (FWER). This approach, though, has faults, and we point out a few. A different approach to problems of multiple significance testing is presented. It calls for controlling the expected proportion of falsely rejected hypotheses-the false discovery rate. This error rate is equivalent to the FWER when all hypotheses are true but is smaller otherwise. Therefore, in problems where the control of the false discovery rate rather than that of the FWER is desired, there is potential for a gain in power. A simple sequential Bonferroni-type procedure is proved to control the false discovery rate for independent test statistics, and a simulation study shows that the gain in power is substantial. The use of the new procedure and the appropriateness of the criterion are illustrated with examples.},
 author = {Yoav Benjamini and Yosef Hochberg},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {289--300},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing},
 volume = {57},
 year = {1995}
}


 @article{SVM_onepercent, title={Application of support vector machines to metabolomics experiments with limited replicates}, volume={10}, DOI={10.1007/s11306-014-0651-0}, number={6}, journal={Metabolomics}, author={Heinemann, Joshua and Mazurie, Aurélien and Tokmina-Lukaszewska, Monika and Beilman, Greg J. and Bothner, Brian}, year={2014}, pages={1121–1128}} 
 
 
  @article{unreasonable_effectiveness, title={The unreasonable effectiveness of data}, volume={24}, DOI={10.1109/mis.2009.36}, number={2}, journal={IEEE Intelligent Systems}, author={Halevy, Alon and Norvig, Peter and Pereira, Fernando}, year={2009}, pages={8–12}} 
  
 @article{very_large_corpora, title={Scaling to very very large corpora for natural language disambiguation}, DOI={10.3115/1073012.1073017}, journal={Proceedings of the 39th Annual Meeting on Association for Computational Linguistics  - ACL '01}, author={Banko, Michele and Brill, Eric}, year={2001}} 
 
  @book{hands_on_ml, place={Beijing}, title={Hands-on machine learning with SCIKIT-LEARN, Keras, and TENSORFLOW: Concepts, tools, and techniques to build intelligent systems}, publisher={O'Reilly}, author={Géron, Aurélien}, year={2020}} 
  
 @book{ESL, place={New York}, title={The elements of statistical learning data mining, inference, and prediction}, publisher={Springer}, author={Hastie, Trevor and Tibshirani, Robert and Friedman, J. H.}, year={2009}} 
 
 @article{fdr_example_1,
title = {Univariate and classification analysis reveals potential diagnostic biomarkers for early stage ovarian cancer Type 1 and Type 2},
journal = {Journal of Proteomics},
volume = {196},
pages = {57-68},
year = {2019},
issn = {1874-3919},
doi = {https://doi.org/10.1016/j.jprot.2019.01.017},
url = {https://www.sciencedirect.com/science/article/pii/S1874391919300259},
author = {Simonas Marcišauskas and Benjamin Ulfenborg and Björg Kristjansdottir and Sofia Waldemarson and Karin Sundfeldt},
keywords = {Ovarian cancer, Cyst fluid, FIGO stage I, Type 1 and Type 2, Proteomics, Proteome, Biomarker, Diagnostics}
}

 @article{efron2011, title={Tweedie’s formula and selection bias}, volume={106}, DOI={10.1198/jasa.2011.tm11181}, number={496}, journal={Journal of the American Statistical Association}, author={Efron, Bradley}, year={2011}, pages={1602–1614}} 
 
  @article{robbins1956, series={1954-1955}, title={An empirical bayes approach to statistics}, volume={I}, DOI={10.1525/9780520313880-015}, journal={Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability}, author={Robbins, Herbert}, year={1956}, pages={157–163}, collection={1954-1955}} 
  
@article{efron2008, title={Microarrays, Empirical Bayes and the Two-Groups Model}, volume={23}, DOI={10.1214/07-sts236}, number={1}, journal={Statistical Science}, author={Efron, Bradley}, year={2008}, pages={1–22}}

@article{LASSO,
 ISSN = {00359246},
 URL = {http://www.jstor.org/stable/2346178},
 abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
 author = {Robert Tibshirani},
 journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
 number = {1},
 pages = {267--288},
 publisher = {[Royal Statistical Society, Wiley]},
 title = {Regression Shrinkage and Selection via the Lasso},
 volume = {58},
 year = {1996}
}

@article{RF,
author = {Breiman L},
journal = {Machine Learning},
pages = {5--32},
title = {Random Forests},
volume = {45},
year = {2001}
}

 @article{simseq, title={Spsimseq: Semi-parametric simulation of bulk and single-cell rna-sequencing data}, volume={36}, DOI={10.1093/bioinformatics/btaa105}, number={10}, journal={Bioinformatics}, author={Assefa, Alemu Takele and Vandesompele, Jo and Thas, Olivier}, year={2020}, pages={3276–3278}} 
 
  @article{dataset, series={43}, title={Global gene expression profiling in Escherichia coli K12: The effects of leucine-responsive regulatory protein}, volume={25}, DOI={doi: 10.1074/jbc.M204044200}, number={277}, journal={J Biol Chem.}, author={Hung, She-pin and Baldi, Pierre and Hatfield, Wesley}, year={2002}, pages={40309–23}, collection={43}} 
  
